{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feavisual.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "CPHZv5LZGGMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cPickle\n",
        "from tensorflow.python.platform import gfile\n",
        "from random import randint\n",
        "import os\n",
        "from scipy.misc import imsave\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTdLqT5sGMRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "  fo = open(file, 'rb')\n",
        "  dict = cPickle.load(fo)\n",
        "  fo.close()\n",
        "  return dict\n",
        "\n",
        "def initWeight(shape):\n",
        "    weights = tf.truncated_normal(shape,stddev=0.1)\n",
        "    return tf.Variable(weights)\n",
        "\n",
        "# start with 0.1 so reLu isnt always 0\n",
        "def initBias(shape):\n",
        "    bias = tf.constant(0.1,shape=shape)\n",
        "    return tf.Variable(bias)\n",
        "\n",
        "# the convolution with padding of 1 on each side, and moves by 1.\n",
        "def conv2d(x,W):\n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\"SAME\")\n",
        "\n",
        "# max pooling basically shrinks it by 2x, taking the highest value on each feature.\n",
        "def maxPool2d(x):\n",
        "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "STb0GarPGPY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "c540753d-43da-4891-8ede-e008fbcfa63f"
      },
      "cell_type": "code",
      "source": [
        "batchsize = 50;\n",
        "imagesize = 32;\n",
        "colors = 3;\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "img = tf.placeholder(\"float\",shape=[None,imagesize,imagesize,colors])\n",
        "lbl = tf.placeholder(\"float\",shape=[None,10])\n",
        "# for each 5x5 area, check for 32 features over 3 color channels\n",
        "wConv1 = initWeight([5,5,colors,32])\n",
        "bConv1 = initBias([32])\n",
        "# move the conv filter over the picture\n",
        "conv1 = conv2d(img,wConv1)\n",
        "# adds bias\n",
        "bias1 = conv1 + bConv1\n",
        "# relu = max(0,x), adds nonlinearality\n",
        "relu1 = tf.nn.relu(bias1)\n",
        "# maxpool to 16x16\n",
        "pool1 = maxPool2d(relu1)\n",
        "# second conv layer, takes a 16x16 with 32 layers, turns to 8x8 with 64 layers\n",
        "wConv2 = initWeight([5,5,32,64])\n",
        "bConv2 = initBias([64])\n",
        "conv2 = conv2d(pool1,wConv2)\n",
        "bias2 = conv2 + bConv2\n",
        "relu2 = tf.nn.relu(bias2)\n",
        "pool2 = maxPool2d(relu2)\n",
        "# fully-connected is just a regular neural net: 8*8*64 for each training data\n",
        "wFc1 = initWeight([(imagesize/4) * (imagesize/4) * 64, 1024])\n",
        "bFc1 = initBias([1024])\n",
        "# reduce dimensions to flatten\n",
        "pool2flat = tf.reshape(pool2, [-1, (imagesize/4) * (imagesize/4) *64])\n",
        "# 128 training set by 2304 data points\n",
        "fc1 = tf.matmul(pool2flat,wFc1) + bFc1;\n",
        "relu3 = tf.nn.relu(fc1);\n",
        "# dropout removes duplicate weights\n",
        "keepProb = tf.placeholder(\"float\");\n",
        "drop = tf.nn.dropout(relu3,keepProb);\n",
        "wFc2 = initWeight([1024,10]);\n",
        "bFc2 = initWeight([10]);\n",
        "# softmax converts individual probabilities to percentages\n",
        "guesses = tf.nn.softmax(tf.matmul(drop, wFc2) + bFc2);\n",
        "# how wrong it is\n",
        "cross_entropy = -tf.reduce_sum(lbl*tf.log(guesses + 1e-9));\n",
        "# theres a lot of tensorflow optimizers such as gradient descent\n",
        "# adam is one of them\n",
        "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy);\n",
        "# array of bools, checking if each guess was correct\n",
        "correct_prediction = tf.equal(tf.argmax(guesses,1), tf.argmax(lbl,1));\n",
        "# represent the correctness as a float [1,1,0,1] -> 0.75\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"));\n",
        "\n",
        "\n",
        "sess.run(tf.initialize_all_variables());\n",
        "\n",
        "batch = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "\n",
        "validationData = batch[\"data\"][555:batchsize+555]\n",
        "validationRawLabel = batch[\"labels\"][555:batchsize+555]\n",
        "validationLabel = np.zeros((batchsize,10))\n",
        "validationLabel[np.arange(batchsize),validationRawLabel] = 1\n",
        "validationData = validationData/255.0\n",
        "validationData = np.reshape(validationData,[-1,3,32,32])\n",
        "validationData = np.swapaxes(validationData,1,3)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, tf.train.latest_checkpoint(os.getcwd()+\"/training/\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4eec5fcb758d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cifar-10-batches-py/data_batch_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mvalidationData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m555\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m555\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1961d3b2368b>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'cifar-10-batches-py/data_batch_1'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IWUVfkECFiNx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train for 20000\n",
        "# print mnistbatch[0].shape\n",
        "def train():\n",
        "    for i in range(20000):\n",
        "        randomint = randint(0,10000 - batchsize - 1)\n",
        "        trainingData = batch[\"data\"][randomint:batchsize+randomint]\n",
        "        rawlabel = batch[\"labels\"][randomint:batchsize+randomint]\n",
        "        trainingLabel = np.zeros((batchsize,10))\n",
        "        trainingLabel[np.arange(batchsize),rawlabel] = 1\n",
        "        trainingData = trainingData/255.0\n",
        "        trainingData = np.reshape(trainingData,[-1,3,32,32])\n",
        "        trainingData = np.swapaxes(trainingData,1,3)\n",
        "\n",
        "        if i%10 == 0:\n",
        "            train_accuracy = accuracy.eval(feed_dict={\n",
        "            img: validationData, lbl: validationLabel, keepProb: 1.0})\n",
        "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
        "\n",
        "            if i%50 == 0:\n",
        "                saver.save(sess, os.getcwd()+\"/training/train\", global_step=i)\n",
        "\n",
        "        optimizer.run(feed_dict={img: trainingData, lbl: trainingLabel, keepProb: 0.5})\n",
        "        print i\n",
        "\n",
        "def unpool(value, name='unpool'):\n",
        "    \"\"\"N-dimensional version of the unpooling operation from\n",
        "    https://www.robots.ox.ac.uk/~vgg/rg/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf\n",
        "\n",
        "    :param value: A Tensor of shape [b, d0, d1, ..., dn, ch]\n",
        "    :return: A Tensor of shape [b, 2*d0, 2*d1, ..., 2*dn, ch]\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name) as scope:\n",
        "        sh = value.get_shape().as_list()\n",
        "        dim = len(sh[1:-1])\n",
        "        out = (tf.reshape(value, [-1] + sh[-dim:]))\n",
        "        for i in range(dim, 0, -1):\n",
        "            out = tf.concat(i, [out, out])\n",
        "        out_size = [-1] + [s * 2 for s in sh[1:-1]] + [sh[-1]]\n",
        "        out = tf.reshape(out, out_size, name=scope)\n",
        "    return out\n",
        "\n",
        "def display():\n",
        "    print \"displaying\"\n",
        "\n",
        "    batchsizeFeatures = 50\n",
        "    imageIndex = 56\n",
        "\n",
        "    inputImage = batch[\"data\"][imageIndex:imageIndex+batchsizeFeatures]\n",
        "    inputImage = inputImage/255.0\n",
        "    inputImage = np.reshape(inputImage,[-1,3,32,32])\n",
        "    inputImage = np.swapaxes(inputImage,1,3)\n",
        "\n",
        "    inputLabel = np.zeros((batchsize,10))\n",
        "    inputLabel[np.arange(1),batch[\"labels\"][imageIndex:imageIndex+batchsizeFeatures]] = 1;\n",
        "    # inputLabel = batch[\"labels\"][54]\n",
        "\n",
        "\n",
        "    # prints a given image\n",
        "\n",
        "\n",
        "    # saves pixel-representations of features from Conv layer 1\n",
        "    featuresReLu1 = tf.placeholder(\"float\",[None,32,32,32])\n",
        "    unReLu = tf.nn.relu(featuresReLu1)\n",
        "    unBias = unReLu\n",
        "    unConv = tf.nn.conv2d_transpose(unBias, wConv1, output_shape=[batchsizeFeatures,imagesize,imagesize,colors] , strides=[1,1,1,1], padding=\"SAME\")\n",
        "    activations1 = relu1.eval(feed_dict={img: inputImage, lbl: inputLabel, keepProb: 1.0})\n",
        "    print np.shape(activations1)\n",
        "\n",
        "    # display features\n",
        "    for i in xrange(32):\n",
        "        isolated = activations1.copy()\n",
        "        isolated[:,:,:,:i] = 0\n",
        "        isolated[:,:,:,i+1:] = 0\n",
        "        print np.shape(isolated)\n",
        "        totals = np.sum(isolated,axis=(1,2,3))\n",
        "        best = np.argmin(totals,axis=0)\n",
        "        print best\n",
        "        pixelactive = unConv.eval(feed_dict={featuresReLu1: isolated})\n",
        "        # totals = np.sum(pixelactive,axis=(1,2,3))\n",
        "        # best = np.argmax(totals,axis=0)\n",
        "        # best = 0\n",
        "        saveImage(pixelactive[best],\"activ\"+str(i)+\".png\")\n",
        "        saveImage(inputImage[best],\"activ\"+str(i)+\"-base.png\")\n",
        "\n",
        "    # display same feature for many images\n",
        "    # for i in xrange(batchsizeFeatures):\n",
        "    #     isolated = activations1.copy()\n",
        "    #     isolated[:,:,:,:6] = 0\n",
        "    #     isolated[:,:,:,7:] = 0\n",
        "    #     pixelactive = unConv.eval(feed_dict={featuresReLu1: isolated})\n",
        "    #     totals = np.sum(pixelactive,axis=(1,2,3))\n",
        "    #     best = np.argmax(totals,axis=0)\n",
        "    #     saveImage(pixelactive[i],\"activ\"+str(i)+\".png\")\n",
        "    #     saveImage(inputImage[i],\"activ\"+str(i)+\"-base.png\")\n",
        "\n",
        "\n",
        "\n",
        "    # saves pixel-representations of features from Conv layer 2\n",
        "    featuresReLu2 = tf.placeholder(\"float\",[None,16,16,64])\n",
        "    unReLu2 = tf.nn.relu(featuresReLu2)\n",
        "    unBias2 = unReLu2\n",
        "    unConv2 = tf.nn.conv2d_transpose(unBias2, wConv2, output_shape=[batchsizeFeatures,imagesize/2,imagesize/2,32] , strides=[1,1,1,1], padding=\"SAME\")\n",
        "    unPool = unpool(unConv2)\n",
        "    unReLu = tf.nn.relu(unPool)\n",
        "    unBias = unReLu\n",
        "    unConv = tf.nn.conv2d_transpose(unBias, wConv1, output_shape=[batchsizeFeatures,imagesize,imagesize,colors] , strides=[1,1,1,1], padding=\"SAME\")\n",
        "    activations1 = relu2.eval(feed_dict={img: inputImage, lbl: inputLabel, keepProb: 1.0})\n",
        "    print np.shape(activations1)\n",
        "\n",
        "    # display features\n",
        "    # for i in xrange(64):\n",
        "    #     isolated = activations1.copy()\n",
        "    #     isolated[:,:,:,:i] = 0\n",
        "    #     isolated[:,:,:,i+1:] = 0\n",
        "    #     pixelactive = unConv.eval(feed_dict={featuresReLu2: isolated})\n",
        "    #     # totals = np.sum(pixelactive,axis=(1,2,3))\n",
        "    #     # best = np.argmax(totals,axis=0)\n",
        "    #     best = 0\n",
        "    #     saveImage(pixelactive[best],\"activ\"+str(i)+\".png\")\n",
        "    #     saveImage(inputImage[best],\"activ\"+str(i)+\"-base.png\")\n",
        "\n",
        "\n",
        "    # display same feature for many images\n",
        "    # for i in xrange(batchsizeFeatures):\n",
        "    #     isolated = activations1.copy()\n",
        "    #     isolated[:,:,:,:8] = 0\n",
        "    #     isolated[:,:,:,9:] = 0\n",
        "    #     pixelactive = unConv.eval(feed_dict={featuresReLu2: isolated})\n",
        "    #     totals = np.sum(pixelactive,axis=(1,2,3))\n",
        "    #     # best = np.argmax(totals,axis=0)\n",
        "    #     # best = 0\n",
        "    #     saveImage(pixelactive[i],\"activ\"+str(i)+\".png\")\n",
        "    #     saveImage(inputImage[i],\"activ\"+str(i)+\"-base.png\")\n",
        "\n",
        "\n",
        "\n",
        "def saveImage(inputImage, name):\n",
        "    # red = inputImage[:1024]\n",
        "    # green = inputImage[1024:2048]\n",
        "    # blue = inputImage[2048:]\n",
        "    # formatted = np.zeros([3,32,32])\n",
        "    # formatted[0] = np.reshape(red,[32,32])\n",
        "    # formatted[1] = np.reshape(green,[32,32])\n",
        "    # formatted[2] = np.reshape(blue,[32,32])\n",
        "    # final = np.swapaxes(formatted,0,2)/255;\n",
        "    final = inputImage\n",
        "    final = np.rot90(np.rot90(np.rot90(final)))\n",
        "    imsave(name,final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOKNaOPtGWfF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(argv=None):\n",
        "    display()\n",
        "    # train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
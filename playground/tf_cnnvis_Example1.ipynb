{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_cnnvis_Example1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eqJW-jHyIOWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3209b8f1-7827-4327-89ad-56af0251e24f"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "lib_url = 'https://raw.githubusercontent.com/abx67/AdvML-style-transfer/master/playground/tf_cnnvis.py'\n",
        "if not os.path.exists('tf_cnnvis.py'):\n",
        "  !curl -O $lib_url\n",
        "lib_url = 'https://raw.githubusercontent.com/abx67/AdvML-style-transfer/master/playground/utils.py'\n",
        "if not os.path.exists('utils.py'):\n",
        "  !curl -O $lib_url\n",
        "img_url = 'https://raw.githubusercontent.com/abx67/AdvML-style-transfer/master/img/van_gogh.jpg'\n",
        "if not os.path.exists('images.jpg'):\n",
        "  !curl -o images.jpg $img_url"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  104k  100  104k    0     0   620k      0 --:--:-- --:--:-- --:--:--  620k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WkdZLXfcHXZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.misc import imread, imresize\n",
        "\n",
        "from tf_cnnvis import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4l6V0Am2HXZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7d9dc265-6d08-4820-c0a9-eea0902f6c03"
      },
      "cell_type": "code",
      "source": [
        "# download alexnet model weights\n",
        "if not os.path.exists(\"alexnet_weights.h5\"):\n",
        "    !curl -o alexnet_weights.h5 http://files.heuritech.com/weights/alexnet_weights.h5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  232M  100  232M    0     0  1109k      0  0:03:34  0:03:34 --:--:--  788k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L_ADf_dLHXZR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loading parameters and mean image for pretrained alexnet model\n",
        "# mean = np.load(\"./img_mean.npy\").transpose((1, 2, 0)) # load mean image of imagenet dataset\n",
        "\n",
        "f = h5py.File('./alexnet_weights.h5','r')\n",
        "\n",
        "conv_1 = [f[\"conv_1\"][\"conv_1_W\"], f[\"conv_1\"][\"conv_1_b\"]]\n",
        "conv_2_1 = [f[\"conv_2_1\"][\"conv_2_1_W\"], f[\"conv_2_1\"][\"conv_2_1_b\"]]\n",
        "conv_2_2 = [f[\"conv_2_2\"][\"conv_2_2_W\"], f[\"conv_2_2\"][\"conv_2_2_b\"]]\n",
        "conv_3 = [f[\"conv_3\"][\"conv_3_W\"], f[\"conv_3\"][\"conv_3_b\"]]\n",
        "conv_4_1 = [f[\"conv_4_1\"][\"conv_4_1_W\"], f[\"conv_4_1\"][\"conv_4_1_b\"]]\n",
        "conv_4_2 = [f[\"conv_4_2\"][\"conv_4_2_W\"], f[\"conv_4_2\"][\"conv_4_2_b\"]]\n",
        "conv_5_1 = [f[\"conv_5_1\"][\"conv_5_1_W\"], f[\"conv_5_1\"][\"conv_5_1_b\"]]\n",
        "conv_5_2 = [f[\"conv_5_2\"][\"conv_5_2_W\"], f[\"conv_5_2\"][\"conv_5_2_b\"]]\n",
        "fc_6 = [f[\"dense_1\"][\"dense_1_W\"], f[\"dense_1\"][\"dense_1_b\"]]\n",
        "fc_7 = [f[\"dense_2\"][\"dense_2_W\"], f[\"dense_2\"][\"dense_2_b\"]]\n",
        "fc_8 = [f[\"dense_3\"][\"dense_3_W\"], f[\"dense_3\"][\"dense_3_b\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKHtoioVHXZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tensorflow model implementation (Alexnet convolution)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape = [None, 224, 224, 3]) # placeholder for input images\n",
        "y_ = tf.placeholder(tf.float32, shape = [None, 1000]) # placeholder for true labels for input images\n",
        "\n",
        "radius = 5; alpha = 1e-4; beta = 0.75; bias = 2.0 # hyper parametes for lrn\n",
        "\n",
        "\n",
        "# Layer - 1 conv1\n",
        "W_conv_1 = tf.Variable(np.transpose(conv_1[0], (2, 3, 1, 0)))\n",
        "b_conv_1 = tf.Variable(np.reshape(conv_1[1], (96, )))\n",
        "\n",
        "y_conv_1 = tf.nn.conv2d(X, filter=W_conv_1, strides=[1, 4, 4, 1], padding=\"SAME\") + b_conv_1\n",
        "h_conv_1 = tf.nn.relu(y_conv_1, name = \"conv1\")\n",
        "h_conv_1 = tf.nn.local_response_normalization(h_conv_1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
        "h_pool_1 = tf.nn.max_pool(h_conv_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "h_pool_1_1, h_pool_1_2 = tf.split(axis = 3, value = h_pool_1, num_or_size_splits = 2)\n",
        "\n",
        "\n",
        "\n",
        "# Layer - 2 conv2\n",
        "W_conv_2_1 = tf.Variable(np.transpose(conv_2_1[0], (2, 3, 1, 0)))\n",
        "b_conv_2_1 = tf.Variable(np.reshape(conv_2_1[1], (128, )))\n",
        "\n",
        "y_conv_2_1 = tf.nn.conv2d(h_pool_1_1, filter=W_conv_2_1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_2_1\n",
        "h_conv_2_1 = tf.nn.relu(y_conv_2_1, name = \"conv2_1\")\n",
        "h_conv_2_1 = tf.nn.local_response_normalization(h_conv_2_1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
        "h_pool_2_1 = tf.nn.max_pool(h_conv_2_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "\n",
        "W_conv_2_2 = tf.Variable(np.transpose(conv_2_2[0], (2, 3, 1, 0)))\n",
        "b_conv_2_2 = tf.Variable(np.reshape(conv_2_2[1], (128, )))\n",
        "\n",
        "y_conv_2_2 = tf.nn.conv2d(h_pool_1_2, filter=W_conv_2_2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_2_2\n",
        "h_conv_2_2 = tf.nn.relu(y_conv_2_2, name = \"conv2_2\")\n",
        "h_conv_2_2 = tf.nn.local_response_normalization(h_conv_2_2, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
        "h_pool_2_2 = tf.nn.max_pool(h_conv_2_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "h_pool_2 = tf.concat(axis = 3, values = [h_pool_2_1, h_pool_2_2])\n",
        "\n",
        "\n",
        "\n",
        "# Layer - 3 conv3\n",
        "W_conv_3 = tf.Variable(np.transpose(conv_3[0], (2, 3, 1, 0)))\n",
        "b_conv_3 = tf.Variable(np.reshape(conv_3[1], (384, )))\n",
        "\n",
        "y_conv_3 = tf.nn.conv2d(h_pool_2, filter=W_conv_3, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_3\n",
        "h_conv_3 = tf.nn.relu(y_conv_3, name = \"conv3\")\n",
        "\n",
        "h_conv_3_1, h_conv_3_2 = tf.split(axis = 3, value = h_conv_3, num_or_size_splits = 2)\n",
        "\n",
        "\n",
        "# h_conv_4_1 = h_conv_3_1\n",
        "# h_conv_4_2 = h_conv_3_2\n",
        "# Layer - 4 conv4\n",
        "W_conv_4_1 = tf.Variable(np.transpose(conv_4_1[0], (2, 3, 1, 0)))\n",
        "b_conv_4_1 = tf.Variable(np.reshape(conv_4_1[1], (192, )))\n",
        "\n",
        "y_conv_4_1 = tf.nn.conv2d(h_conv_3_1, filter=W_conv_4_1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_4_1\n",
        "h_conv_4_1 = tf.nn.relu(y_conv_4_1, name = \"conv4_1\")\n",
        "\n",
        "\n",
        "W_conv_4_2 = tf.Variable(np.transpose(conv_4_2[0], (2, 3, 1, 0)))\n",
        "b_conv_4_2 = tf.Variable(np.reshape(conv_4_2[1], (192, )))\n",
        "\n",
        "y_conv_4_2 = tf.nn.conv2d(h_conv_3_2, filter=W_conv_4_2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_4_2\n",
        "h_conv_4_2 = tf.nn.relu(y_conv_4_2, name = \"conv4_2\")\n",
        "\n",
        "h_conv_4 = tf.concat(axis = 3, values = [h_conv_4_1, h_conv_4_2])\n",
        "\n",
        "\n",
        "\n",
        "# Layer - 5 conv5\n",
        "W_conv_5_1 = tf.Variable(np.transpose(conv_5_1[0], (2, 3, 1, 0)))\n",
        "b_conv_5_1 = tf.Variable(np.reshape(conv_5_1[1], (128, )))\n",
        "\n",
        "y_conv_5_1 = tf.nn.conv2d(h_conv_4_1, filter=W_conv_5_1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_5_1\n",
        "h_conv_5_1 = tf.nn.relu(y_conv_5_1, name = \"conv5_1\")\n",
        "h_conv_5_1 = tf.nn.local_response_normalization(h_conv_5_1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
        "h_pool_5_1 = tf.nn.max_pool(h_conv_5_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "\n",
        "W_conv_5_2 = tf.Variable(np.transpose(conv_5_2[0], (2, 3, 1, 0)))\n",
        "b_conv_5_2 = tf.Variable(np.reshape(conv_5_2[1], (128, )))\n",
        "\n",
        "y_conv_5_2 = tf.nn.conv2d(h_conv_4_2, filter=W_conv_5_2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_5_2\n",
        "h_conv_5_2 = tf.nn.relu(y_conv_5_2, name = \"conv5_2\")\n",
        "h_conv_5_2 = tf.nn.local_response_normalization(h_conv_5_2, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
        "h_pool_5_2 = tf.nn.max_pool(h_conv_5_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "\n",
        "h_pool_5 = tf.concat(axis = 3, values = [h_pool_5_1, h_pool_5_2])\n",
        "\n",
        "dimensions = h_pool_5.get_shape().as_list()\n",
        "dim = dimensions[1] * dimensions[2] * dimensions[3]\n",
        "\n",
        "# # Part of Alexnet model which is not required for deconvolution\n",
        "h_flatten = tf.reshape(h_pool_5, shape=[-1, dim])\n",
        "\n",
        "# Layer - 6 fc6\n",
        "W_full_6 = tf.Variable(np.array(fc_6[0]))\n",
        "b_full_6 = tf.Variable(np.array(fc_6[1]))\n",
        "\n",
        "y_full_6 = tf.add(tf.matmul(h_flatten, W_full_6), b_full_6)\n",
        "h_full_6 = tf.nn.relu(y_full_6, name = \"fc6\")\n",
        "h_dropout_6 = tf.nn.dropout(h_full_6, 0.5)\n",
        "\n",
        "\n",
        "# Layer - 7 fc7\n",
        "W_full_7 = tf.Variable(np.array(fc_7[0]))\n",
        "b_full_7 = tf.Variable(np.array(fc_7[1]))\n",
        "\n",
        "y_full_7 = tf.add(tf.matmul(h_dropout_6, W_full_7), b_full_7)\n",
        "h_full_7 = tf.nn.relu(y_full_7, name = \"fc7\")\n",
        "h_dropout_7 = tf.nn.dropout(h_full_7, 0.5)\n",
        "\n",
        "\n",
        "# Layer - 8 fc8\n",
        "W_full_8 = tf.Variable(np.array(fc_8[0]))\n",
        "b_full_8 = tf.Variable(np.array(fc_8[1]))\n",
        "\n",
        "y_full_8 = tf.add(tf.matmul(h_dropout_7, W_full_8), b_full_8, name = \"fc8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zibuE0SpHXZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "011569d0-3859-4c3e-abb5-c4a21ee8f8ea"
      },
      "cell_type": "code",
      "source": [
        "# reading sample image\n",
        "im = np.expand_dims(imresize(imresize(imread(os.path.join(\"images.jpg\")), (256, 256)) - 0, \n",
        "                             (224, 224)), axis = 0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fN4k_qtyHXZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# open a session and initialize graph variables\n",
        "# CAVEAT: trained alexnet weights have been set as initialization values in the graph nodes.\n",
        "#         For this reason visualization can be performed just after initialization\n",
        "sess = tf.Session(graph=tf.get_default_graph())\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8TPXCkvKHXZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "92d19a58-6188-4f72-a88e-d761507df837"
      },
      "cell_type": "code",
      "source": [
        "# activation visualization\n",
        "layers = ['r', 'p', 'c']\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "# with sess_graph_path = None, the default Session will be used for visualization.\n",
        "    is_success = activation_visualization(sess_graph_path = None, value_feed_dict = {X : im}, \n",
        "                                          layers=layers, path_logdir=os.path.join(\"Log\",\"AlexNet\"), \n",
        "                                          path_outdir=os.path.join(\"Output\",\"AlexNet\"))\n",
        "start = time.time() - start\n",
        "print(\"Total Time = %f\" % (start))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/tmp-model\n",
            "Reconstruction Completed for conv1 layer. Time taken = 0.136796 s\n",
            "Reconstruction Completed for conv2_1 layer. Time taken = 0.058276 s\n",
            "Reconstruction Completed for conv2_2 layer. Time taken = 0.054345 s\n",
            "Reconstruction Completed for conv3 layer. Time taken = 0.066663 s\n",
            "Reconstruction Completed for conv4_1 layer. Time taken = 0.061105 s\n",
            "Reconstruction Completed for conv4_2 layer. Time taken = 0.055337 s\n",
            "Reconstruction Completed for conv5_1 layer. Time taken = 0.048215 s\n",
            "Reconstruction Completed for conv5_2 layer. Time taken = 0.047259 s\n",
            "Skipping. Too many featuremaps. May cause memory errors.\n",
            "Skipping. Too many featuremaps. May cause memory errors.\n",
            "Reconstruction Completed for MaxPool layer. Time taken = 0.044867 s\n",
            "Reconstruction Completed for MaxPool_1 layer. Time taken = 0.045511 s\n",
            "Reconstruction Completed for MaxPool_2 layer. Time taken = 0.036309 s\n",
            "Reconstruction Completed for MaxPool_3 layer. Time taken = 0.047520 s\n",
            "Reconstruction Completed for MaxPool_4 layer. Time taken = 0.046922 s\n",
            "Reconstruction Completed for Conv2D layer. Time taken = 0.087597 s\n",
            "Reconstruction Completed for Conv2D_1 layer. Time taken = 0.055971 s\n",
            "Reconstruction Completed for Conv2D_2 layer. Time taken = 0.047777 s\n",
            "Reconstruction Completed for Conv2D_3 layer. Time taken = 0.056468 s\n",
            "Reconstruction Completed for Conv2D_4 layer. Time taken = 0.048818 s\n",
            "Reconstruction Completed for Conv2D_5 layer. Time taken = 0.049155 s\n",
            "Reconstruction Completed for Conv2D_6 layer. Time taken = 0.046414 s\n",
            "Reconstruction Completed for Conv2D_7 layer. Time taken = 0.046693 s\n",
            "Total Time = 4.243486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R64AEVvcHXZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "53d718fe-5999-4e9f-cec9-e4200559b47b"
      },
      "cell_type": "code",
      "source": [
        "# deconv visualization\n",
        "layers = ['r', 'p', 'c']\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    is_success = deconv_visualization(sess_graph_path = None, value_feed_dict = {X : im}, \n",
        "                                      layers=layers, path_logdir=os.path.join(\"Log\",\"AlexNet\"), \n",
        "                                      path_outdir=os.path.join(\"Output\",\"AlexNet\"))\n",
        "start = time.time() - start\n",
        "print(\"Total Time = %f\" % (start))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/tmp-model\n",
            "Reconstruction Completed for conv1 layer. Time taken = 3.364138 s\n",
            "Reconstruction Completed for conv2_1 layer. Time taken = 9.121234 s\n",
            "Reconstruction Completed for conv2_2 layer. Time taken = 7.453517 s\n",
            "Reconstruction Completed for conv3 layer. Time taken = 27.815069 s\n",
            "Reconstruction Completed for conv4_1 layer. Time taken = 14.243843 s\n",
            "Reconstruction Completed for conv4_2 layer. Time taken = 15.144454 s\n",
            "Reconstruction Completed for conv5_1 layer. Time taken = 8.196763 s\n",
            "Reconstruction Completed for conv5_2 layer. Time taken = 9.603134 s\n",
            "Skipping. Too many featuremaps. May cause memory errors.\n",
            "Skipping. Too many featuremaps. May cause memory errors.\n",
            "Reconstruction Completed for MaxPool layer. Time taken = 4.691796 s\n",
            "Reconstruction Completed for MaxPool_1 layer. Time taken = 9.621528 s\n",
            "Reconstruction Completed for MaxPool_2 layer. Time taken = 8.401789 s\n",
            "Reconstruction Completed for MaxPool_3 layer. Time taken = 9.063271 s\n",
            "Reconstruction Completed for MaxPool_4 layer. Time taken = 10.270351 s\n",
            "Reconstruction Completed for Conv2D layer. Time taken = 3.201365 s\n",
            "Reconstruction Completed for Conv2D_1 layer. Time taken = 10.242117 s\n",
            "Reconstruction Completed for Conv2D_2 layer. Time taken = 8.562514 s\n",
            "Reconstruction Completed for Conv2D_3 layer. Time taken = 26.891135 s\n",
            "Reconstruction Completed for Conv2D_4 layer. Time taken = 14.402369 s\n",
            "Reconstruction Completed for Conv2D_5 layer. Time taken = 14.912884 s\n",
            "Reconstruction Completed for Conv2D_6 layer. Time taken = 11.070612 s\n",
            "Reconstruction Completed for Conv2D_7 layer. Time taken = 11.919017 s\n",
            "Total Time = 241.521010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JotBg2AEHXZg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#close the session and release variables\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AZVCSnffHXZm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
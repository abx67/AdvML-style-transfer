{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of main_debug.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/abx67/AdvML-style-transfer/blob/master/main_debug.ipynb","timestamp":1543724511889}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"rPpZmbiidgtZ","colab_type":"text"},"cell_type":"markdown","source":["# Main Debug Notebook"]},{"metadata":{"id":"ajbA42EhXNtq","colab_type":"text"},"cell_type":"markdown","source":["#### download weight and libraries\n"]},{"metadata":{"id":"eQa8yTQ1A70q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":395},"outputId":"d0b70670-5833-47c5-e0b6-d4367e45ee4a","executionInfo":{"status":"ok","timestamp":1543988578865,"user_tz":300,"elapsed":33764,"user":{"displayName":"Shiyu Liu","photoUrl":"","userId":"10508464269545444574"}}},"cell_type":"code","source":["import os\n","\n","weight_folder = './data/'\n","lib_folder = './src/'\n","img_folder = './img/'\n","if not os.path.exists(weight_folder): \n","  os.mkdir(weight_folder)\n","if not os.path.exists(img_folder): \n","  os.mkdir(img_folder)\n","if not os.path.exists(lib_folder): \n","  os.mkdir(lib_folder)\n","\n","# Download weight\n","weight_url = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n","weight_path = weight_folder + 'imagenet-vgg-verydeep-19.mat'\n","if not os.path.exists(weight_path):\n","  !curl -o $weight_path $weight_url\n","\n","# Download libraries\n","libraries = ['vgg.py', 'constants.py', 'neural_network.py', 'utils.py']\n","library_url = 'https://raw.githubusercontent.com/abx67/AdvML-style-transfer/master/src/'\n","\n","for lib in libraries:\n","  lib_path = lib_folder + lib\n","  lib_url = library_url + lib\n","  if not os.path.exists(lib_path):\n","    !curl -o $lib_path $lib_url\n","    \n","# Download images\n","style_img_name = 'van_gogh.jpg'\n","content_img_name = 'new_york.jpg'\n","img_url = 'https://raw.githubusercontent.com/abx67/AdvML-style-transfer/master/img/'\n","\n","style_path = img_folder + style_img_name\n","style_url = img_url + style_img_name\n","content_path = img_folder + content_img_name\n","content_url = img_url + content_img_name\n","\n","if not os.path.exists(style_path):\n","  !curl -o $style_path $style_url\n","if not os.path.exists(content_path):\n","  !curl -o $content_path $content_url"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  510M  100  510M    0     0  20.6M      0  0:00:24  0:00:24 --:--:-- 21.9M\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  2470  100  2470    0     0   4891      0 --:--:-- --:--:-- --:--:--  4891\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100   779  100   779    0     0   1719      0 --:--:-- --:--:-- --:--:--  1715\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  7101  100  7101    0     0  39232      0 --:--:-- --:--:-- --:--:-- 39232\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100   674  100   674    0     0   1375      0 --:--:-- --:--:-- --:--:--  1375\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  104k  100  104k    0     0   489k      0 --:--:-- --:--:-- --:--:--  489k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  120k  100  120k    0     0   157k      0 --:--:-- --:--:-- --:--:--  157k\n"],"name":"stdout"}]},{"metadata":{"id":"S8hpK52gbU9O","colab_type":"text"},"cell_type":"markdown","source":["#### import modules"]},{"metadata":{"id":"WyJAmez0bZQ7","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","import numpy as np\n","import PIL.Image\n","import sys\n","sys.path.append(lib_folder)\n","# from neural_network import NeuralNetwork\n","from vgg import VGG"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oHBKl9yxb1pd","colab_type":"text"},"cell_type":"markdown","source":["#### contents definition"]},{"metadata":{"id":"5BPKDOEcb4Am","colab_type":"code","colab":{}},"cell_type":"code","source":["VGG_MAT_PATH = weight_path\n","CONTENT_IMAGE_PATH = content_path\n","STYLE_IMAGE_PATH = style_path\n","MIXED_IMAGE_PATH = './output/'\n","if not os.path.exists(MIXED_IMAGE_PATH): \n","  os.mkdir(MIXED_IMAGE_PATH)\n","\n","CONTENT_WEIGHT = 5\n","STYLE_WEIGHT = 50000\n","VARIATION_WEIGHT = 10000\n","LEARNING_RATE = 10\n","BETA1 = 0.9\n","BETA2 = 0.999\n","EPSILON = 1e-08\n","MAX_ITERATION = 1000\n","POOLING = 'avg'\n","CHECK_POINT = 50"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P7g6A46QcMfd","colab_type":"text"},"cell_type":"markdown","source":["#### function definition"]},{"metadata":{"id":"w9Fs1kUkcLbj","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_image(file_path, max_size=None, shape=None):\n","    # load image and define the factor used to tranfer the image size\n","    image = PIL.Image.open(file_path)\n","    # resize by max_size\n","    if max_size is not None:\n","        factor = float(max_size) / np.max(image.size)  # image.size = [height, width, 3]\n","        size = np.array(image.size) * factor\n","        size = size.astype(int)\n","        image = image.resize(size, PIL.Image.LANCZOS)  # image resize with filter LANCZOS\n","    # resize with shape\n","    if shape is not None:\n","        image = image.resize(shape, PIL.Image.LANCZOS)\n","    # return image values with float data type\n","    return np.float32(image)\n","\n","\n","def save_image(file_path, image):\n","    # ensure the pixel value is int between 0 and 255\n","    image = np.clip(image, 0.0, 255.0).astype(np.uint8)\n","    # write to file\n","    PIL.Image.fromarray(image).save(file_path)\n","    return\n","\n","\n","def style_transfer(content_image_path, style_image_path, mixed_image_path,\n","                   content_weight, style_weight, variation_weight,\n","                   pooling, learning_rate, beta1, beta2, epsilon, max_iteration, check_point):\n","    # set the time point\n","    time_start = time.time()\n","\n","    # load image\n","    content_image = load_image(content_image_path)\n","    style_image = load_image(style_image_path, shape=content_image.shape[:2])\n","\n","    # initialize object\n","    vgg = VGG(VGG_MAT_PATH, pooling)\n","    nn = NeuralNetwork(content_image, style_image, vgg, content_weight, style_weight, variation_weight)\n","\n","    # train the model\n","    for i, mixed_image in nn.train_model(learning_rate, beta1, beta2, epsilon, max_iteration, check_point):\n","        save_image(mixed_image_path + 'v1_{}.jpeg'.format(i + 1), mixed_image)\n","\n","    # print time\n","    time_end = time.time()\n","    print('Time elapsed: {} seconds'.format(round(time_end - time_start)))\n","\n","    return"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3ioabxYdX8y9","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import os\n","from functools import reduce\n","\n","PATH = '../'\n","\n","CONTENT_LAYER_WEIGHTS = {\n","    'relu4_2': 1.0,\n","}\n","\n","STYLE_LAYER_WEIGHTS = {\n","    'relu1_1': 0.2,\n","    'relu2_1': 0.2,\n","    'relu3_1': 0.2,\n","    'relu4_1': 0.2,\n","    'relu5_1': 0.2,\n","}\n","\n","\n","class NeuralNetwork(object):\n","    # neural network used for style transfer, which includes the definition of loss function, optimization function, etc.\n","    def __init__(self, content, style, vgg, content_weight, style_weight, variation_weight):\n","        # content - image, shape = (height, width, 3)\n","        # style - image, shape = (height, width, 3)\n","        # vgg - vgg object, definition see vgg.py\n","        # content_weight - scalar, weight for the loss of the content image\n","        # style_weight - scalar, weight for the loss of the style image\n","        # variation_weight - scalar, weight for the loss of variation of the mixed image\n","\n","        self.content = content\n","        self.style = style\n","        self.vgg = vgg\n","\n","        self.content_weight = content_weight\n","        self.style_weight = style_weight\n","        self.variation_weight = variation_weight\n","\n","        self.content_shape = (1,) + self.content.shape\n","        self.style_shape = (1,) + self.style.shape\n","\n","        self.content_layer_weights = CONTENT_LAYER_WEIGHTS\n","        self.style_layer_weights = STYLE_LAYER_WEIGHTS\n","\n","        self.content_features = self.get_content_features()\n","        self.style_features = self.get_style_features()\n","\n","        return\n","\n","    def get_content_features(self):\n","        content_features = {}\n","        graph = tf.Graph()\n","        with graph.as_default(), graph.device('/cpu:0'), tf.Session() as sess:\n","            image = tf.placeholder('float', shape=self.content_shape)\n","            net = self.vgg.load_net(image)\n","            content = np.array(self.content - self.vgg.mean_pix)  # de-mean\n","            content = np.reshape(content, (1,) + content.shape)\n","            for layer_name in self.content_layer_weights:\n","                content_features[layer_name] = net[layer_name].eval(feed_dict={image: content})\n","        print(content_features['relu4_2'].shape)\n","        return content_features\n","\n","    def get_style_features(self):\n","        style_features = {}\n","        graph = tf.Graph()\n","        with graph.as_default(), graph.device('/cpu:0'), tf.Session() as sess:\n","            image = tf.placeholder('float', shape=self.style_shape)\n","            net = self.vgg.load_net(image)\n","            style = np.array(self.style - self.vgg.mean_pix)  # de-mean\n","            style = np.reshape(style, (1,) + style.shape)\n","            for layer_name in self.style_layer_weights:\n","                features = net[layer_name].eval(feed_dict={image: style})\n","                features = np.reshape(features, (-1, features.shape[3]))\n","                gram = features.T.dot(features) / features.size  # TODO: find out why divide by the size\n","                style_features[layer_name] = gram\n","        return style_features\n","\n","    def train_model(self, learning_rate, beta1, beta2, epsilon, max_iteration, check_point, init_image='content'):\n","        with tf.Graph().as_default():\n","            # initial image with random guess\n","            noise = np.random.normal(size=self.content_shape, scale=np.std(self.content) * 0.1)  # useless\n","            if init_image == 'random':\n","                init_image = tf.random_normal(self.content_shape)\n","            elif init_image == 'content':\n","                init_image = np.reshape(np.array(self.content - self.vgg.mean_pix), self.content_shape)\n","            mixed_image = tf.Variable(init_image, dtype=tf.float32)\n","            mixed_net = self.vgg.load_net(mixed_image)\n","\n","            # calculate loss\n","            loss_content = self.calculate_loss_content(mixed_net)\n","            loss_style = self.calculate_loss_style(mixed_net)\n","            loss_variation = self.calculate_loss_variation(mixed_image)\n","            loss_total = loss_content + loss_style + loss_variation\n","\n","            # summary statistics\n","            tf.summary.scalar('loss_content', loss_content)\n","            tf.summary.scalar('loss_style', loss_style)\n","            tf.summary.scalar('loss_variation', loss_variation)\n","            tf.summary.scalar('loss_total', loss_total)\n","            summary_loss = tf.summary.merge_all()\n","\n","            # initialize optimization\n","            train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss_total)\n","\n","            with tf.Session() as sess:\n","                summary_writer = tf.summary.FileWriter(PATH + 'logs', sess.graph)\n","                sess.run(tf.global_variables_initializer())\n","\n","                for i in range(max_iteration):\n","                    train_step.run()\n","                    summary = sess.run(summary_loss)\n","                    summary_writer.add_summary(summary, i)\n","                    # save image\n","                    if (check_point and ((i + 1) % check_point) == 0) or i == max_iteration - 1:\n","                        image_out = mixed_image.eval()\n","                        image_out = image_out.reshape(self.content_shape[1:]) + self.vgg.mean_pix\n","                        print('iter: {}, loss total: {}, loss content: {}, loss style: {}, loss variation: {}'.format(\n","                            i + 1, loss_total.eval(), loss_content.eval(), loss_style.eval(), loss_variation.eval()\n","                        ))\n","                        yield i, image_out\n","            return mixed_image\n","\n","    def calculate_loss_content(self, mixed_net):\n","        losses = []\n","        for layer_name in self.content_layer_weights:\n","            losses += [self.content_layer_weights[layer_name] * 2 * tf.nn.l2_loss(\n","                mixed_net[layer_name] - self.content_features[layer_name]) / self.content_features[\n","                           layer_name].size]  # TODO: find out why divide by the size\n","        return self.content_weight * reduce(tf.add, losses)\n","\n","    def calculate_loss_style(self, mixed_net):\n","        losses = []\n","        for layer_name in self.style_layer_weights:\n","            _, height, width, channel = mixed_net[layer_name].get_shape()\n","            size = height.value * width.value * channel.value\n","            mixed_features = tf.reshape(mixed_net[layer_name], (-1, channel.value))\n","            mixed_gram = tf.matmul(tf.transpose(mixed_features),\n","                                   mixed_features) / size  # TODO: find out why divide by the size\n","            losses += [self.style_layer_weights[layer_name] * 2 * tf.nn.l2_loss(\n","                mixed_gram - self.style_features[\n","                    layer_name]) / self.style_features[layer_name].size]  # TODO: find out why divide by the size\n","        return self.style_weight * reduce(tf.add, losses)\n","\n","    def calculate_loss_variation(self, mixed_image):\n","        height_size = np.prod([dim.value for dim in mixed_image[:, 1:, :, :].get_shape()])\n","        width_size = np.prod([dim.value for dim in mixed_image[:, :, 1:, :].get_shape()])\n","        \n","        loss = 2 * (tf.nn.l2_loss(\n","            mixed_image[:, 1:, :, :] - mixed_image[:, :mixed_image.shape[1] - 1, :, :]) / height_size + tf.nn.l2_loss(\n","            mixed_image[:, :, 1:, :] - mixed_image[:, :, :mixed_image.shape[2] - 1, :]) / width_size)\n","        #loss_poisson = tf.nn.log_poisson_loss(\n","        height_diff = mixed_image[:,1:, :, :]-mixed_image[:,:mixed_image.shape[1]-1, :, :]\n","        weight_diff = mixed_image[:, :, 1:, :]-mixed_image[:, :, :mixed_image.shape[2]-1, :]\n","        ## Threshold for variation loss detection\n","        threshold = 100.0\n","        height_diff_k = tf.less(tf.math.abs(height_diff), tf.constant(threshold))\n","        height_diff_k = tf.boolean_mask(height_diff, height_diff_k)\n","        weight_diff_k = tf.less(tf.math.abs(weight_diff), tf.constant(threshold))\n","        weight_diff_k = tf.boolean_mask(weight_diff, weight_diff_k)\n","\n","        #loss = 2 * (tf.nn.l2_loss(height_diff_k) / height_size + tf.nn.l2_loss(weight_diff_k) / width_size)\n","        \n","        \n","        h = 0.05\n","        height_kernel = tf.math.exp(-1*tf.math.abs(height_diff))/h\n","        weight_kernel = tf.math.exp(-1*tf.math.abs(weight_diff))/h\n","        #loss = 2 * (tf.nn.l2_loss(height_kernel) / height_size + tf.nn.l2_loss(weight_kernel) / width_size)\n","        return self.variation_weight * loss\n","      \n","    #def kernel_loss(self, mixed_imageï¼Œ threshold):\n","     #   diff =   (mixed_image[:, 1:, :, :] - mixed_image[:, :mixed_image.shape[1] - 1, :, :]) \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HZ02uFR-6yjM","colab_type":"code","outputId":"35fbe9ec-4c35-48ab-a3f1-b74a6cf829f4","executionInfo":{"status":"ok","timestamp":1543988634190,"user_tz":300,"elapsed":26314,"user":{"displayName":"Shiyu Liu","photoUrl":"","userId":"10508464269545444574"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#TEST CODE for Variation LOSS\n","content_image = load_image(CONTENT_IMAGE_PATH)\n","style_image = load_image(STYLE_IMAGE_PATH, shape=content_image.shape[:2])\n","vgg = VGG(VGG_MAT_PATH, pooling = \"avg\")\n","nn1 = NeuralNetwork(content_image, style_image, vgg, 5, 5000, 1000)\n","#mixed_image = tf.Variable(init_image, dtype=tf.float32)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(1, 95, 71, 512)\n"],"name":"stdout"}]},{"metadata":{"id":"FSlu1OSe0ZQN","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","init_image = np.reshape(np.array(content_image - vgg.mean_pix), content_image.shape)\n","mixed_image = tf.Variable(init_image, dtype=tf.float32)\n","init = tf.global_variables_initializer()\n","\n","\n","height_diff = mixed_image[1:, :, :]-mixed_image[:mixed_image.shape[0]-1, :, :]\n","weight_diff = mixed_image[:, 1:, :]-mixed_image[:, :mixed_image.shape[1]-1, :]\n","## Threshold for variation loss detection\n","threshold = 100.0\n","height_diff_k = tf.less(tf.math.abs(height_diff), tf.constant(threshold))\n","height_diff_k = tf.boolean_mask(height_diff, height_diff_k)\n","weight_diff_k = tf.less(tf.math.abs(weight_diff), tf.constant(threshold))\n","weight_diff_k = tf.boolean_mask(weight_diff, weight_diff_k)\n","\n","with tf.Session() as sess:\n","  sess.run(init)\n","  print(sess.run(height_diff))\n","  print(sess.run(height_diff_k))\n","  print(sess.run(tf.shape(mixed_image)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E93aexEF52eM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":899},"outputId":"ed814872-4f5a-43a8-f219-741138dbe6c8","executionInfo":{"status":"ok","timestamp":1543989585847,"user_tz":300,"elapsed":408,"user":{"displayName":"Shiyu Liu","photoUrl":"","userId":"10508464269545444574"}}},"cell_type":"code","source":["# h is the parameter \n","h = 0.05\n","height_kernel = tf.math.exp(-1*tf.math.abs(height_diff))/h\n","\n","with tf.Session() as sess:\n","  sess.run(init)\n","  print(sess.run(height_kernel))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[[8.2798749e-07 8.2798749e-07 8.2798749e-07]\n","  [3.0459958e-07 3.0459958e-07 3.0459958e-07]\n","  [3.0459958e-07 3.0459958e-07 3.0459958e-07]\n","  ...\n","  [3.0459958e-07 3.0459958e-07 3.0459958e-07]\n","  [1.1205592e-07 1.1205592e-07 1.1205592e-07]\n","  [1.5165121e-08 1.1205592e-07 1.1205635e-07]]\n","\n"," [[4.9575042e-02 6.7092525e-03 1.3475893e-01]\n","  [4.9575042e-02 6.7092525e-03 1.8237639e-02]\n","  [1.8237639e-02 1.8237639e-02 1.8237639e-02]\n","  ...\n","  [1.8237639e-02 2.4681960e-03 6.7092525e-03]\n","  [1.8237639e-02 2.4681960e-03 6.7092525e-03]\n","  [1.8237639e-02 2.4681960e-03 6.7092525e-03]]\n","\n"," [[2.7067056e+00 2.7067056e+00 2.7067056e+00]\n","  [2.7067056e+00 2.7067056e+00 3.6631280e-01]\n","  [2.7067056e+00 2.7067056e+00 3.6631280e-01]\n","  ...\n","  [2.7067056e+00 7.3575888e+00 2.0000000e+01]\n","  [2.0000000e+01 2.0000000e+01 2.0000000e+01]\n","  [2.0000000e+01 2.0000000e+01 2.0000000e+01]]\n","\n"," ...\n","\n"," [[1.8237639e-02 1.8237639e-02 1.8237639e-02]\n","  [1.3475893e-01 1.3475893e-01 1.3475893e-01]\n","  [9.9574137e-01 9.9574137e-01 9.9574137e-01]\n","  ...\n","  [2.4681960e-03 2.4681960e-03 2.4681960e-03]\n","  [6.7092525e-03 6.7092525e-03 6.7092525e-03]\n","  [4.9575042e-02 4.9575042e-02 4.9575042e-02]]\n","\n"," [[9.9574137e-01 9.9574137e-01 9.9574137e-01]\n","  [1.3475893e-01 1.3475893e-01 1.3475893e-01]\n","  [6.7092525e-03 6.7092525e-03 6.7092525e-03]\n","  ...\n","  [2.7067056e+00 2.7067056e+00 2.7067056e+00]\n","  [2.0000000e+01 2.0000000e+01 2.0000000e+01]\n","  [2.7067056e+00 2.7067056e+00 2.7067056e+00]]\n","\n"," [[9.9574137e-01 9.9574137e-01 9.9574137e-01]\n","  [3.6631280e-01 3.6631280e-01 3.6631280e-01]\n","  [1.3475893e-01 1.3475893e-01 1.3475893e-01]\n","  ...\n","  [2.7067056e+00 2.7067056e+00 2.7067056e+00]\n","  [7.3575888e+00 7.3575888e+00 7.3575888e+00]\n","  [7.3575888e+00 7.3575888e+00 7.3575888e+00]]]\n"],"name":"stdout"}]},{"metadata":{"id":"Q4_KvOV1cQ-F","colab_type":"text"},"cell_type":"markdown","source":["#### main function"]},{"metadata":{"id":"yqa12dkmcO_Y","colab_type":"code","outputId":"57b28aea-6433-49c9-f277-800b1415cce2","executionInfo":{"status":"ok","timestamp":1543991441145,"user_tz":300,"elapsed":628656,"user":{"displayName":"Shiyu Liu","photoUrl":"","userId":"10508464269545444574"}},"colab":{"base_uri":"https://localhost:8080/","height":413}},"cell_type":"code","source":["if __name__ == '__main__':\n","    style_transfer(content_image_path=CONTENT_IMAGE_PATH,\n","                   style_image_path=STYLE_IMAGE_PATH,\n","                   mixed_image_path=MIXED_IMAGE_PATH,\n","                   content_weight=CONTENT_WEIGHT,\n","                   style_weight=STYLE_WEIGHT,\n","                   variation_weight=VARIATION_WEIGHT,\n","                   pooling=POOLING,\n","                   learning_rate=LEARNING_RATE,\n","                   beta1=BETA1,\n","                   beta2=BETA2,\n","                   epsilon=EPSILON,\n","                   max_iteration=MAX_ITERATION,\n","                   check_point=CHECK_POINT)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["(1, 95, 71, 512)\n","iter: 50, loss total: 12251283.0, loss content: 415448.46875, loss style: 2207239.25, loss variation: 9628595.0\n","iter: 100, loss total: 10431272.0, loss content: 410810.0625, loss style: 1582027.375, loss variation: 8438435.0\n","iter: 150, loss total: 9972294.0, loss content: 406106.875, loss style: 1393345.5, loss variation: 8172842.0\n","iter: 200, loss total: 9734897.0, loss content: 403270.28125, loss style: 1308593.375, loss variation: 8023033.0\n","iter: 250, loss total: 9590807.0, loss content: 400963.6875, loss style: 1267129.625, loss variation: 7922713.5\n","iter: 300, loss total: 9489703.0, loss content: 400169.78125, loss style: 1246756.875, loss variation: 7842776.0\n","iter: 350, loss total: 9410239.0, loss content: 399445.53125, loss style: 1235977.75, loss variation: 7774815.5\n","iter: 400, loss total: 9378101.0, loss content: 397331.0, loss style: 1252562.875, loss variation: 7728207.0\n","iter: 450, loss total: 9391082.0, loss content: 398946.5625, loss style: 1255935.0, loss variation: 7736201.0\n","iter: 500, loss total: 9267650.0, loss content: 398228.53125, loss style: 1232742.625, loss variation: 7636678.5\n","iter: 550, loss total: 9254186.0, loss content: 397615.625, loss style: 1224854.125, loss variation: 7631716.5\n","iter: 600, loss total: 9258745.0, loss content: 397011.46875, loss style: 1257617.875, loss variation: 7604116.0\n","iter: 650, loss total: 9339624.0, loss content: 398967.15625, loss style: 1283245.0, loss variation: 7657412.0\n","iter: 700, loss total: 9178880.0, loss content: 397618.78125, loss style: 1222547.375, loss variation: 7558713.5\n","iter: 750, loss total: 9641903.0, loss content: 400051.78125, loss style: 1614846.625, loss variation: 7627004.5\n","iter: 800, loss total: 9503499.0, loss content: 396989.0625, loss style: 1464546.5, loss variation: 7641963.5\n","iter: 850, loss total: 9218985.0, loss content: 393686.90625, loss style: 1329356.0, loss variation: 7495942.5\n","iter: 900, loss total: 9515761.0, loss content: 389134.6875, loss style: 1620016.125, loss variation: 7506610.0\n","iter: 950, loss total: 9250977.0, loss content: 396488.6875, loss style: 1307905.125, loss variation: 7546583.5\n","iter: 1000, loss total: 9126945.0, loss content: 394583.21875, loss style: 1248954.125, loss variation: 7483407.5\n","Time elapsed: 628 seconds\n"],"name":"stdout"}]},{"metadata":{"id":"Ns6gbBzHcSq5","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
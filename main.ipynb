{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_colorbot_generate_starter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "CmO1WEDPp1xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3sd_AEJspQJg",
        "colab_type": "code",
        "outputId": "3c54cfaa-2607-4b0a-9a1d-d8156381f2b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the colors dataset\n",
        "if not os.path.exists('colors.csv'):\n",
        "  !curl -O 'https://raw.githubusercontent.com/random-forests/datasets/master/colors.csv'\n",
        "!head colors.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name,red,green,blue\n",
            "parakeet,174,182,87\n",
            "saddle brown,88,52,1\n",
            "cucumber crush,222,237,215\n",
            "pool blue,134,194,201\n",
            "distance,98,110,130\n",
            "light urple,179,111,246\n",
            "east side,172,145,206\n",
            "florida seashells,250,228,199\n",
            "paris,145,167,189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mG0WnNZtmxul",
        "colab_type": "code",
        "outputId": "ff7652fd-30f3-428a-c260-b9b2e54606c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "colors_rgb = []\n",
        "csv_reader = csv.reader(open('colors.csv'), delimiter=',')\n",
        "next(csv_reader) # Remove the header\n",
        "for row in csv_reader:\n",
        "    name, r, g, b = row[0].lower().strip(), int(row[1]), int(row[2]), int(row[3])\n",
        "    colors_rgb.append((name, r, g, b))\n",
        "print(len(colors_rgb), 'colors downloaded')\n",
        "print('For example', colors_rgb[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14157 colors downloaded\n",
            "For example ('parakeet', 174, 182, 87)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PdIWQdlBrZXA",
        "colab_type": "code",
        "outputId": "f78cad02-4d3c-471f-f4b7-03dca6cdeb2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# In this experiment, we will train a char-based RNN to generate a line of text\n",
        "# that resembles this dataset (we'll treat each line as a string)\n",
        "sentences = []\n",
        "for row in colors_rgb:\n",
        "  line = ' '.join([str(part) for part in row])\n",
        "  sentences.append(line)\n",
        "print(sentences[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parakeet 174 182 87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q_o2oIUNuyWZ",
        "colab_type": "code",
        "outputId": "9b2a4d69-9a94-4072-dd2d-7c192ecdb1e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# vocabulary for our char-based RNN\n",
        "chars = set()\n",
        "for sentence in sentences:\n",
        "  for char in sentence:\n",
        "    chars.add(char)\n",
        "    \n",
        "# add a special char for padding\n",
        "chars.add('<pad>')\n",
        "\n",
        "vocab = sorted(set(chars))\n",
        "\n",
        "# Create a mapping from unique characters to indices\n",
        "char2idx = {u : i for i, u in enumerate(vocab)}\n",
        "idx2char = {i : u for i, u in enumerate(vocab)}\n",
        "\n",
        "# Vocab size\n",
        "vocab_size = len(vocab)\n",
        "print('vocab size:', vocab_size)\n",
        "print(vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 38\n",
            "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p5sdKRJ6rp8I",
        "colab_type": "code",
        "outputId": "266d08d9-f1e9-4187-a6b6-ab1c7734c9fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# vectorize the text\n",
        "text_int = []\n",
        "rgb_int = []\n",
        "for sentence in sentences:\n",
        "  int_sentence = [] \n",
        "  for c in sentence:\n",
        "    int_sentence.append(char2idx[c])\n",
        "  text_int.append(int_sentence)\n",
        "print('Vectorized sentence', text_int[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorized sentence [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0g6Do15As4yQ",
        "colab_type": "code",
        "outputId": "78441ccb-bd28-40f5-956b-5d2cb43413ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# pad sentences to max_length\n",
        "max_length = 40\n",
        "for sentence in text_int:\n",
        "  while (len(sentence) < max_length):\n",
        "    sentence.append(char2idx['<pad>'])\n",
        "print('Padded sentences', text_int[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded sentences [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-y87WF_Fw1kJ",
        "colab_type": "code",
        "outputId": "82e4fbda-142d-4231-eb5b-731c4fc34ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# truncate all sentences to max_length\n",
        "for i in range(len(text_int)):\n",
        "  sentence = text_int[i]\n",
        "  if len(sentence) > max_length:\n",
        "    text_int[i] = sentence[:max_length]\n",
        "print(\"Truncated sentences\", text_int[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncated sentences [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwu3FgSpxNWT",
        "colab_type": "code",
        "outputId": "8a477eb1-70da-479e-d3ec-b621c6701389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# Create training examples / targets\n",
        "input_text = []\n",
        "target_text = []\n",
        "\n",
        "for i in range(len(text_int)):\n",
        "  inps = text_int[i][:max_length-1]\n",
        "  targ = text_int[i][1:max_length]\n",
        "  input_text.append(inps)\n",
        "  target_text.append(targ)\n",
        "  \n",
        "print(\"First training example, target\")  \n",
        "print(input_text[0])\n",
        "print(target_text[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First training example, target\n",
            "[27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
            "[12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cjyeDrvGzl8E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TT8ed7cu0w-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Model, self).__init__()\n",
        "    self.units = units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.GRU = tf.keras.layers.GRU(units=units, return_sequences=True, recurrent_initializer='glorot_uniform',\n",
        "                                  stateful=True)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x):\n",
        "    embedding = self.embedding(x)\n",
        "    GRU = self.GRU(embedding)\n",
        "    prediction = self.fc(GRU)\n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jA8Pssrh1NKE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "# Here, this is basically just a trick to avoid having \n",
        "# to one-hot encode the characters\n",
        "# I don't think it will add much otherwise\n",
        "# this would be more useful if we had a much larger vocabulary\n",
        "embedding_dim = 128\n",
        "\n",
        "# Number of RNN units\n",
        "units = 256\n",
        "\n",
        "model = Model(vocab_size, embedding_dim, units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrSRbSQk1Rcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(labels, logits):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-AILeTt71UXr",
        "colab_type": "code",
        "outputId": "2f9c1537-64ed-421b-a2da-9b77406640fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model.build(tf.TensorShape([BATCH_SIZE, max_length]))\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  4864      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  295680    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  9766      \n",
            "=================================================================\n",
            "Total params: 310,310\n",
            "Trainable params: 310,310\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gjYtdulr8ukK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "# Checkpoint instance\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkLMi8GI1c9b",
        "colab_type": "code",
        "outputId": "498c600c-8611-47d0-deeb-7967ba4bcff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    # initally hidden is None\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (input_seq, target_seq)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              predictions = model(input_seq)\n",
        "              loss = loss_function(target_seq, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time for epoch {} sec\\n'.format(time.time() - start))\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.6555\n",
            "Epoch 1 Batch 100 Loss 1.5199\n",
            "Epoch 1 Batch 200 Loss 1.1438\n",
            "Epoch 1 Loss 1.1607\n",
            "Time for epoch 142.6920154094696 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4192\n",
            "Epoch 2 Batch 100 Loss 1.1020\n",
            "Epoch 2 Batch 200 Loss 1.1186\n",
            "Epoch 2 Loss 1.0661\n",
            "Time for epoch 137.24009585380554 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1201\n",
            "Epoch 3 Batch 100 Loss 1.0379\n",
            "Epoch 3 Batch 200 Loss 1.0016\n",
            "Epoch 3 Loss 1.0450\n",
            "Time for epoch 137.30580687522888 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0452\n",
            "Epoch 4 Batch 100 Loss 0.9996\n",
            "Epoch 4 Batch 200 Loss 0.9762\n",
            "Epoch 4 Loss 1.0206\n",
            "Time for epoch 136.18550992012024 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.0335\n",
            "Epoch 5 Batch 100 Loss 0.9751\n",
            "Epoch 5 Batch 200 Loss 0.9960\n",
            "Epoch 5 Loss 0.9255\n",
            "Time for epoch 136.50782823562622 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.9753\n",
            "Epoch 6 Batch 100 Loss 0.9257\n",
            "Epoch 6 Batch 200 Loss 0.8796\n",
            "Epoch 6 Loss 0.9190\n",
            "Time for epoch 135.96480417251587 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9847\n",
            "Epoch 7 Batch 100 Loss 0.9003\n",
            "Epoch 7 Batch 200 Loss 0.9146\n",
            "Epoch 7 Loss 0.8873\n",
            "Time for epoch 133.616064786911 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.9169\n",
            "Epoch 8 Batch 100 Loss 0.8782\n",
            "Epoch 8 Batch 200 Loss 0.9038\n",
            "Epoch 8 Loss 0.8854\n",
            "Time for epoch 136.80253767967224 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9379\n",
            "Epoch 9 Batch 100 Loss 0.8567\n",
            "Epoch 9 Batch 200 Loss 0.8425\n",
            "Epoch 9 Loss 0.8555\n",
            "Time for epoch 133.35822987556458 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.9065\n",
            "Epoch 10 Batch 100 Loss 0.8194\n",
            "Epoch 10 Batch 200 Loss 0.8335\n",
            "Epoch 10 Loss 0.8235\n",
            "Time for epoch 134.32104587554932 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0U_cmqGP8dwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2c030fde-8d4c-4015-e01c-aa460fe178ce"
      },
      "cell_type": "code",
      "source": [
        "!ls {checkpoint_dir}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t     ckpt-5.data-00000-of-00001\n",
            "ckpt-10.data-00000-of-00001  ckpt-5.index\n",
            "ckpt-10.index\t\t     ckpt-6.data-00000-of-00001\n",
            "ckpt-1.data-00000-of-00001   ckpt-6.index\n",
            "ckpt-1.index\t\t     ckpt-7.data-00000-of-00001\n",
            "ckpt-2.data-00000-of-00001   ckpt-7.index\n",
            "ckpt-2.index\t\t     ckpt-8.data-00000-of-00001\n",
            "ckpt-3.data-00000-of-00001   ckpt-8.index\n",
            "ckpt-3.index\t\t     ckpt-9.data-00000-of-00001\n",
            "ckpt-4.data-00000-of-00001   ckpt-9.index\n",
            "ckpt-4.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0K3VLTVT81rt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is a hack to let us use the model with a different \n",
        "# batch size later\n",
        "model = Model(vocab_size, embedding_dim, units)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Le_MRIFI1tRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "1f857502-678c-4696-d2ec-f20fc4ea421d"
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step (generating text using the learned model)\n",
        "\n",
        "# Number of characters to generate\n",
        "num_generate = max_length\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = random.choice(string.ascii_lowercase)\n",
        "\n",
        "# Converting our start string to numbers (vectorizing) \n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# Empty string to store our results\n",
        "text_generated = []\n",
        "\n",
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "temperature = 0.5\n",
        "\n",
        "# Here batch size == 1\n",
        "model.reset_states()\n",
        "print(input_eval)\n",
        "for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a multinomial distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "generated_color = start_string + ''.join(text_generated).replace('<pad>', '')\n",
        "print(generated_color)\n",
        "\n",
        "try:\n",
        "  parts = generated_color.split()\n",
        "  r = float(parts[-3])\n",
        "  g = float(parts[-2])\n",
        "  b = float(parts[-1])\n",
        "  plt.clf()\n",
        "  _ = plt.imshow([[(r, g, b)]])\n",
        "  _ = plt.axis('off')\n",
        "  _ = plt.title(generated_color, fontsize=18)\n",
        "except:\n",
        "  print('unable to parse color')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[24]], shape=(1, 1), dtype=int32)\n",
            "moushard 221 192 188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAFdCAYAAABcnZV9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6f44ec0898>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "eQa8yTQ1A70q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}